/*
 * Copyright (c) 2018, Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 */

//===----------------------------------------------------------------------===//
//
// This file defines all of the GenX-specific intrinsics, which correspond to
// vISA instructions.
//
// Comment lines with a triple slash /// introduction are extracted and
// appended to docs/Targets/GenX/GenXLangRef.rst to give the GenX backend
// language reference in docs/autogenerated/Targets/GenX/GenXLangRef.rst.
//
//===----------------------------------------------------------------------===//

let TargetPrefix = "genx" in {  // All intrinsics start with "llvm.genx.".

  //--------------------------------------------------------------------
  // Start and end markers of the genx intrinsic enum values. This relies on
  // tablegen outputting the intrinsics in sorted by name order.
  def int_genx_aaaabegin : Intrinsic<[llvm_anyvector_ty], [], []>;
  def int_genx_zzzzend : Intrinsic<[llvm_anyvector_ty], [], []>;

  // --------------------------------
  /// Region/element access intrinsics
  /// --------------------------------
  ///
  /// ``llvm.genx.rdregion*`` : read a region, direct or single-indirect
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.rdregioni`` : integer element type (not i1)
  /// * ``llvm.genx.rdregionf`` : fp element type
  ///
  /// * arg0: vector to read region out of
  /// * arg1: i32 vstride in elements, constant
  /// * arg2: i32 width in elements, constant
  /// * arg3: i32 stride in elements, constant
  /// * arg4: i16 or vXi16 offset in bytes
  /// * arg5: i32 parent width, constant, ignored if offset is constant
  ///
  /// * Return value: the region extracted
  ///
  /// The return type must be a vector with the same element type as the input
  /// vector, and number of elements giving the total size of the region.
  /// A scalar can be used instead of a 1-vector.
  ///
  /// There are two variants, an integer one and an fp one, because the
  /// intrinsic declaration language does not let us declare the return type
  /// as any scalar or vector int or fp type.
  ///
  /// The element type must be an integral power of two number of bytes up to
  /// and including 8 bytes in size, thus one of i8, i16, i32, i64, half,
  /// float, double. In particular i1 is not allowed.
  /// The width must be non-zero and must divide the total size evenly.
  ///
  /// There is no requirement on vstride, width, stride or total size being
  /// a power of two or having any maximum.
  ///
  /// The offset in bytes arg can be i16 or vector of i16. If a vector, then
  /// its vector width must be the height of the region, i.e. the total
  /// size of the region divided by the width.
  ///
  /// The parent width arg is ignored if the offset arg is constant. If the
  /// offset arg is variable, then a non-undef parent width is a statement
  /// that the value of offset is such that a row of the region does not
  /// cross a multiple of parent width boundary. This is used by the backend
  /// to determine whether the region can be collapsed into another region.
  ///
  def int_genx_rdregioni : Intrinsic<[llvm_anyint_ty], [llvm_anyvector_ty,
      llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_i32_ty],
      [IntrNoMem]>;
  def int_genx_rdregionf : Intrinsic<[llvm_anyfloat_ty], [llvm_anyvector_ty,
      llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_i32_ty],
      [IntrNoMem]>;

  /// ``llvm.genx.wrregion*`` : write a region, direct or single-indirect
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.wrregioni`` : integer element type (not i1)
  /// * ``llvm.genx.wrregionf`` : fp element type
  ///
  /// * arg0: vector to write region in to
  /// * arg1: subvector or scalar to write into the region
  /// * arg2: i32 vstride in elements, constant
  /// * arg3: i32 width in elements, constant
  /// * arg4: i32 stride in elements, constant
  /// * arg5: i16 or vXi16 offset in bytes
  /// * arg6: i32 parent width, constant, ignored if offset is constant
  /// * arg7: vector of i1 mask, or scalar i1
  ///
  /// * Return value: the updated vector with the region modified
  ///
  /// The return type must be a vector with the same type as the arg0 vector.
  /// The arg1 subvector must have the same element type as the arg0 vector
  /// and be no larger. Arg1 can be a scalar if the number of elements in
  /// the subregion is 1.
  ///
  /// There are two variants, an integer one and an fp one, because the
  /// intrinsic declaration language does not let us declare the arg1 type
  /// as any scalar or vector int or fp type.
  ///
  /// The element type must be an integral power of two number of bytes up to
  /// and including 8 bytes in size, thus one of i8, i16, i32, i64, half,
  /// float, double. In particular i1 is not allowed.
  /// The width must be non-zero and must divide the total size evenly.
  ///
  /// The arg7 mask is a vector of booleans, exactly as wide as the
  /// arg1 subvector, such that an element of the subvector is written into
  /// its place in the vector only if the corresponding element of the mask
  /// is true.
  /// Alternatively, arg7 can be a single i1 constant with value 1,
  /// meaning that the wrregion is unconditional.
  ///
  /// There is no requirement on vstride, width, stride or total size being
  /// a power of two or having any maximum.
  ///
  /// The offset in bytes arg can be i16 or vector of i16. If a vector, then
  /// its vector width must be the height of the region, i.e. the total
  /// size of the region divided by the width.
  ///
  /// After lowering, the arg1 subvector to write can be a scalar of the same
  /// type as an element of arg0, indicating that the region has one element.
  /// (Lowering lowers an insertelement to this type of wrregion.)
  ///
  /// The parent width arg is ignored if the offset arg is constant. If the
  /// offset arg is variable, then a non-undef parent width is a statement
  /// that the value of offset is such that a row of the region does not
  /// cross a multiple of parent width boundary. This is used by the backend
  /// to determine whether the region can be collapsed into another region.
  ///
  def int_genx_wrregioni : Intrinsic<[llvm_anyvector_ty], [LLVMMatchType<0>,
      llvm_anyint_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
      llvm_i32_ty, llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_wrregionf : Intrinsic<[llvm_anyvector_ty], [LLVMMatchType<0>,
      llvm_anyfloat_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
      llvm_i32_ty, llvm_anyint_ty], [IntrNoMem]>;

  /// ``llvm.genx.vstore`` : store a vector value into memory
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// This intrinsic has the exact semantics of an llvm store instruction.
  /// It is designed for reading and writing a pass-by-reference argument
  /// and it stops llvm optimizations from optimizing away accesses to the
  /// pass-by-reference arguments.
  ///
  /// * arg0: the memory to be accessed
  ///
  def int_genx_vstore : Intrinsic<[],
                                  [llvm_anyvector_ty, LLVMAnyPointerType<LLVMMatchType<1>>],
                                  []>;

  /// ``llvm.genx.vload`` : load a vector value from memory
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// This intrinsic has the exact semantics of an llvm load instruction.
  /// It is designed for reading and writing a pass-by-reference argument
  /// and it stops llvm optimizations from optimizing away accesses to the
  /// pass-by-reference arguments.
  ///
  /// * arg0: the memory to be accessed
  /// * Return value: the vector value read
  ///
  def int_genx_vload  : Intrinsic<[llvm_anyvector_ty],
                                  [LLVMAnyPointerType<LLVMMatchType<0>>],
                                  []>;

  // ------------------------------
  /// ALU type conversion intrinsics
  /// ------------------------------

  /// ``llvm.genx.fptosi.sat`` : convert floating point to signed integer with saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: converted value, any scalar or vector integer type
  ///               (treated as signed) with same vector width as arg0
  ///
  def int_genx_fptosi_sat :
              Intrinsic<[llvm_anyint_ty], [llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.fptoui.sat`` : convert floating point to unsigned integer with saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: converted value, any scalar or vector integer type
  ///               (treated as unsigned) with same vector width as arg0
  ///
  def int_genx_fptoui_sat :
              Intrinsic<[llvm_anyint_ty], [llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.sat`` : floating point saturate
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: value to saturate, any scalar or vector floating point type
  ///
  /// * Return value: saturated value, same type as arg0
  ///
  /// We represent floating point saturation by simply calling this intrinsic
  /// on the result of a floating point operation. This works because the
  /// value before saturation fits in the same type.
  ///
  /// We do not have an equivalent for integer saturation, because the
  /// before-saturation value needs a bigger integer type than the result.
  /// Instead, any integer operation that supports saturation needs an
  /// intrinsic for the saturating variant.
  ///
  def int_genx_sat :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*trunc.sat`` : integer truncation with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sstrunc.sat`` : signed result, signed operand
  /// * ``llvm.genx.sutrunc.sat`` : signed result, unsigned operand
  /// * ``llvm.genx.ustrunc.sat`` : unsigned result, signed operand
  /// * ``llvm.genx.uutrunc.sat`` : unsigned result, unsigned operand
  ///
  /// * arg0: value to truncate, any scalar or vector integer type
  ///
  /// * Return value: truncated value, any scalar or vector integer type
  ///               with same vector width as arg0
  ///
  def int_genx_sstrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_sutrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_ustrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;
  def int_genx_uutrunc_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;

  // -------------------
  /// Modifier intrinsics
  /// -------------------
  ///
  /// Abs is the only source modifier that is represented
  /// by an intrinsic; neg(x) uses 0-x, and not(x) uses x^-1.
  ///
  /// ``llvm.genx.abs*`` : take absolute value
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.absf`` : abs modifier for fp
  /// * ``llvm.genx.absi`` : abs modifier for integer
  ///
  /// * arg0: input value, scalar/vector
  ///
  /// * Return value: result, same type
  ///
  def int_genx_absf :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_absi :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // ----------------------------
  /// Boolean reduction intrinsics
  /// ----------------------------

  /// ``llvm.genx.all`` : true if all input elements are true
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value: v*i1
  ///
  /// * Return value: i1 result
  ///
  def int_genx_all : Intrinsic<[llvm_i1_ty], [llvm_anyint_ty], [IntrNoMem]>;

  /// ``llvm.genx.any`` : true if any input element is true
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value: v*i1
  ///
  /// * Return value: i1 result
  ///
  def int_genx_any : Intrinsic<[llvm_i1_ty], [llvm_anyint_ty], [IntrNoMem]>;

   // ----------------------------
  /// SIMD control flow intrinsics
  /// ----------------------------
  /// 
  /// ``goto`` and ``join`` instructions are represented by ``llvm.genx.simdcf.goto``
  /// and ``llvm.genx.simdcf.join`` intrinsics.
  /// 
  /// The BSpec model
  /// ^^^^^^^^^^^^^^^
  /// 
  /// The BSpec defines SIMD control flow in terms of each of the 32 channels
  /// having a PcIP (per-channel instruction pointer), which determines where a
  /// disabled channel will be re-enabled:
  /// 
  /// * A goto has two targets, UIP (update IP) and JIP (join IP).
  ///   
  ///   - A (forward) goto evaluates its vector condition, and, for each channel
  ///     that is enabled and the condition is true, it sets the channel's PcIP to
  ///     UIP, to mark that the channel is disabled until execution reaches the
  ///     join instruction at UIP. If, after disabling channels in this way, no
  ///     channels are left enabled, then execution jumps to JIP.
  /// 
  ///     UIP and JIP may be different, as there may be channels already disabled
  ///     from an earlier goto with their PcIPs set to an earlier point than the
  ///     present goto's UIP. So JIP needs to be set to the earliest point that
  ///     a channel could have its PcIP pointing at.
  /// 
  ///   - There is also a backward goto variant for use in a conditional loop
  ///     back edge (end of a do..while loop). It works the same as a forward goto
  ///     over an unconditional jump back to the top of the loop.
  /// 
  /// * A join has one target, JIP. It reenables all channels that have PcIP set
  ///   to this join. If there are still no channels enabled, it jumps to JIP.
  /// 
  /// * Each instruction's register write-back is gated by which channels are
  ///   enabled, unless the instruction has a nomask bit set. This is in addition
  ///   to optionally being gated by a predicate.
  /// 
  /// * The action of the channel enable mask (and predicate) in a send depends
  ///   on the shared function. Some (e.g. gather and scatter) have the expected
  ///   semantics where disabled channels do not participate in the memory read/write,
  ///   and (in the case of a read) do not update that channel's result.
  /// 
  /// This scheme allows arbitrarily unstructured SIMD control flow. For it to work
  /// and guarantee convergence, it is sufficient (not sure if it is necessary)
  /// for there to be a linear chain of join points, and each goto/join's UIP and
  /// JIP are forward in the chain, and JIPs are set correctly so it is not possible
  /// for execution to "miss out" a join point where a channel should have been
  /// enabled. (As above, a backward goto is handled in this
  /// model by being considered a forward goto over a backward unconditional jump.)
  /// 
  /// In Gen code, this linear chain of join points does not actually have to be in
  /// program order, as long as the join point order with forward UIP and JIP is
  /// derivable.
  /// 
  /// In vISA, the linear chain of join points does have to be in program order.
  /// vISA does not encode the JIP of a goto/join; instead it derives it itself.
  /// Also, vISA uses whether a goto's target is before or after to encode whether
  /// it is a conditional loop backedge branch.
  /// 
  /// The LLVM IR model
  /// ^^^^^^^^^^^^^^^^^
  /// 
  /// The model we use in LLVM IR is very similar to the above.
  /// 
  /// The PcIP (per-channel instruction pointer) is replaced by:
  /// 
  /// * a global (in the function) EM (execution mask), with each channel having a
  ///   bit that is 1 when the channel is enabled;
  /// 
  /// * each join point has a RM (resume mask), with each channel having a bit
  ///   that is 1 if the channel is disabled and due to be re-enabled when execution
  ///   reaches that join point.
  /// 
  /// A goto is represented by the ``llvm.genx.simdcf.goto`` intrinsic. Its
  /// inputs are the current EM value, the current RM value for its UIP, and the
  /// vector condition. Its results are the updated EM value, the updated RM
  /// value for its UIP, and a scalar bool that says whether all channels are now
  /// disabled and execution should branch to the JIP. This last result is then
  /// (usually) used in a standard LLVM conditional ``br`` instruction.
  /// 
  /// A goto is implicitly attached to its UIP join by the input and output RM
  /// values being part of a web of RM values connected by goto and phi nodes
  /// and used in that join.
  /// 
  /// A join is represented by the ``llvm.genx.simdcf.join`` intrinsic. Its
  /// inputs are the current EM value and the current RM value for this join.
  /// Its results are the updated EM value (this join's RM value is now effectively
  /// all zeros so it not returned as a result), and a scalar bool that says whether
  /// all channels are still disabled and execution should branch to the JIP.
  /// This last result is then (optionally) used in a standard LLVM conditional
  /// ``br`` instruction.
  /// 
  /// An instruction's register write-back being gated by which channels are enabled
  /// is modeled by the current EM value (or the appropriate size left slice of it)
  /// being used as the predicate in a select or wrregion or shared function
  /// intrinsic.
  /// 
  /// Note that EM is always 32 bit, but a join's RM may be smaller as it has the same
  /// vector width as the condition on all gotos that update it.
  /// 
  /// This model is equivalent to the BSpec model, as long as:
  /// 
  /// * there is only ever one EM value live at a time with an initial value in a
  ///   function of either all ones or the passed in call mask;
  /// 
  /// * for each join point, there is only ever one RM value live at a time with an
  ///   initial value in a function of all zeros, and a value after the join point of
  ///   all zeros;
  /// 
  /// * it is possible to re-order the code such that the "false" target of a
  ///   conditional branch that a goto or join is attached to is fall-through, and
  ///   all JIPs and UIPs are forward.
  /// 
  /// Like any other variable with multiple values transformed to SSA, different
  /// EM values may be joined with a phi node. Similarly, for a particular join point's
  /// RM, different RM values may be joined with a phi node.
  /// 
  /// The  ``llvm.genx.simdcf.goto`` and ``llvm.genx.simdcf.join`` intrinsics can
  /// only be generated to ``goto`` and ``join`` instructions if the GenX backend
  /// deems them to be used in a way that is equivalent to the BSpec model. Otherwise,
  /// they are lowered to equivalent but slower code that implements the semantics
  /// of the spec of the intrinsics below.
  /// 
  /// There are more detailed requirements on the use of these intrinsics to be able
  /// to generate them to ``goto`` and ``join`` instructions documented in the
  /// GenXSimdCFConformance pass.
  /// 
  /// ``llvm.genx.simdcf.goto`` : goto instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: OldEM (old execution mask): v32i1
  /// * arg1: OldRM (old resume mask): vector of i1
  /// * arg2: SimdCond (the SIMD control flow condition): same type as arg1
  ///
  /// Return value: struct with the following elements:
  ///
  /// * ret0: NewEM (updated execution mask): v32i1
  /// * ret1: NewRM (updated resume mask): same type as arg1
  /// * ret2: BranchCond: i1
  ///
  /// The elements of the returned struct are calculated as follows:
  ///
  /// * NewEM = OldEM & (SimdCond one extended to v16i1)
  /// * NewRM = OldRM | (OldEM & ~(SimdCond & (OldEM truncated to size of SimdCond)))
  /// * BranchCond = !any(NewEM truncated to size of SimdCond)
  /// 
  /// ``llvm.genx.simdcf.goto`` represents a Gen goto instruction, taking a
  /// vector condition, modifying the global EM and the UIP's RM, and
  /// resulting in a scalar condition to be used in a conditional branch whose
  /// "true" successor is the goto's JIP.
  /// 
  /// If the BranchCond result is not used, then the goto's JIP is set to the
  /// join immediately after.
  /// 
  /// If the BranchCond result is used in a conditional branch, and JIP is
  /// later than the earliest join point
  /// where a channel would be re-enabled, then it is undefined whether the
  /// resulting goto instruction's JIP is as specified here, or an earlier join
  /// point. (This rule is to allow for the vISA finalizer re-deriving the JIPs.)
  /// 
  /// If the goto intrinsic's conditional branch simply branches over an empty block
  /// with an unconditional branch, then the GenX backend takes the intrinsic and
  /// the two branches to be a do..while back edge, giving a Gen ``goto``
  /// instruction with BranchCtrl=1, UIP set to the successor of the unconditional
  /// branch (the top of the do..while loop), and JIP set to the following join
  /// instruction.
  /// 
  /// Channels already disabled in EM remain disabled. For enabled channels,
  /// any channel whose element in SimdCond is true becomes disabled in EM, and
  /// the corresponding bit in RM is set such that the channel becomes re-enabled
  /// upon reaching the RM's join point. If all channels in EM are then disabled,
  /// then BranchCond is true and the conditional branch in which it is used
  /// branches to the next join point in sequence.
  /// 
  /// Note that SimdCond has the same sense as in the Gen goto instruction, but
  /// the opposite sense to that in a vISA forward goto instruction.
  ///
  def int_genx_simdcf_goto :
              Intrinsic<[llvm_anyvector_ty, llvm_anyvector_ty, llvm_i1_ty],
                        [LLVMMatchType<0>, LLVMMatchType<1>, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.simdcf.join`` : join instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: OldEM (old execution mask): v32i1
  /// * arg1: RM (resume mask): vector of i1
  ///
  /// Return value: struct with the following elements:
  ///
  /// * ret0: NewEM (updated execution mask): v32i1
  /// * ret1: BranchCond: i1
  ///
  /// The elements of the returned struct are calculated as follows:
  ///
  /// * NewEM = OldEM | (RM zero extended to v32i1)
  /// * BranchCond = !any(NewEM truncated to size of RM)
  ///
  /// This is marked as having side effects to stop LLVM removing an otherwise
  /// unused join at an outer endif.
  ///
  /// ``llvm.genx.simdcf.join`` represents a Gen join instruction, using the join
  /// point's RM, modifying the global EM, and resulting in a scalar condition to
  /// be used (optionally) in a conditional branch whose "true" successor is
  /// the join's JIP.
  /// 
  /// If the BranchCond result is not used, then the join's JIP is undefined; this
  /// case is used when it is known that at least one channel is enabled after
  /// the join so JIP will never be used.
  /// 
  /// If the BranchCond result is used in a conditional branch, and JIP is
  /// later than the earliest join point
  /// where a channel would be re-enabled, then it is undefined whether the
  /// resulting goto instruction's JIP is as specified here, or an earlier join
  /// point. (This rule is to allow for the vISA finalizer re-deriving the JIPs.)
  /// 
  /// Note that vISA does not have a join instruction; the vISA finalizer
  /// recovers the join points from the goto instructions assuming a linear order.
  /// 
  /// Channels with a set bit in RM become enabled in EM. If all channels in EM are
  /// still disabled, then BranchCond is true and the conditional branch in which it
  /// is used branches to the next join point in sequence.
  /// 
  def int_genx_simdcf_join :
              Intrinsic<[llvm_anyvector_ty, llvm_i1_ty],
                        [LLVMMatchType<0>, llvm_anyvector_ty], []>;

  // --------------
  /// ALU intrinsics
  /// --------------

  /// add
  /// ^^^
  /// Non-saturating add intrinsic is not needed. A vISA non-saturating add
  /// where the result type is different to the operand type is represented
  /// by trunc/zext/sext of each operand and then an LLVM IR Add instruction.
  ///

  /// ``llvm.genx.*add.sat`` : add instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssadd.sat`` : result signed, operands signed
  /// * ``llvm.genx.suadd.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usadd.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uuadd.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width
  ///
  /// For an fp add, use the LLVM IR FAdd instruction, followed by
  /// llvm.genx.sat if saturation is required.
  ///
  def int_genx_ssadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuadd_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// addc
  /// ^^^^
  /// No intrinsic for addc as it has two results.
  ///

  /// asr
  /// ^^^
  /// asr intrinsic is not needed. Because asr cannot overflow, an asr that
  /// saturates with a smaller result type than the execution type can be
  /// represented by an LLVM IR Asr instruction then an llvm.genx.sstrunc.sat.
  ///

  /// ``llvm.genx.*avg`` : integer averaging, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssavg`` : result signed, operands signed
  /// * ``llvm.genx.suavg`` : result signed, operands unsigned
  /// * ``llvm.genx.usavg`` : result unsigned, operands signed
  /// * ``llvm.genx.uuavg`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type (not i64)
  ///               with same vector width
  ///
  def int_genx_ssavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuavg : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*avg.sat`` : integer averaging with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssavg.sat`` : result signed, operands signed
  /// * ``llvm.genx.suavg.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usavg.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uuavg.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type (not i64)
  ///               with same vector width
  ///
  def int_genx_ssavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_suavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uuavg_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*bfe`` : bitfield extract
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sbfe`` : bitfield extract, signed result
  /// * ``llvm.genx.ubfe`` : bitfield extract, unsigned result
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_sbfe : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
                LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ubfe : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
                LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.bfi`` : bitfield insert
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as arg0
  /// * arg3: fourth input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_bfi : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>,
        LLVMMatchType<0>, LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.bfrev`` : reverse bits
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector i32 type
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_bfrev : Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.cbit`` : count set bits
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector integer type
  ///
  /// * Return value: result, int32 of same width as arg0
  ///
  def int_genx_cbit : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty], [IntrNoMem]>;

  /// cmp
  /// ^^^
  /// No intrinsic needed as the LLVM IR ICmp and FCmp instructions cover
  /// vISA functionality
  ///

  /// ``llvm.genx.cos`` : cos instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_cos :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// div
  /// ^^^
  /// No intrinsic needed as the LLVM IR SDiv, UDiv and FDiv instructions
  /// cover vISA functionality
  ///

  /// ``llvm.genx.ieee.div`` : Divide, IEEE variant
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector float/double type
  /// * arg1: second input, same type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_ieee_div : Intrinsic<[llvm_anyfloat_ty],
              [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dp2`` : dp2 instruction (dot product on groups of 4 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp2 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dp3`` : dp3 instruction (dot product on groups of 3 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp3 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dp4`` : dp4 instruction (dot product on groups of 4 elements)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dp4 :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.dph`` : dph instruction (dot product homogenous)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_dph :
              Intrinsic<[llvm_anyfloat_ty],
                    [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.exp`` : base 2 exponent
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_exp :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*fbh`` : find bit high
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sfbh`` : find bit high, signed operand
  /// * ``llvm.genx.ufbh`` : find bit high, unsigned operand
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sfbh :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ufbh :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.fbl`` : find bit low
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_fbl :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.frc`` : fractional part
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_frc :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.inv`` : reciprocal
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_inv :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.line`` : linear equation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, vector float with exactly 4 elements
  /// * arg1: second input value, vector float with a multiple of 4 elements
  ///
  /// * Return value: result, same type as arg1
  ///
  def int_genx_line : Intrinsic<[llvm_anyfloat_ty],
                    [llvm_v4f32_ty, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.log`` : base 2 logarithm
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_log :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.lrp`` : linear interpolation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, any vector float with a multiple of 4 elements
  /// * arg1: second input value, same type as arg0
  /// * arg2: third input value, same type as arg0
  ///
  /// * Return value: result, same type
  ///
  def int_genx_lrp : Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>,
              LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.lzd`` : leading zero detection
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector i32 type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_lzd :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*mad`` : mad instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmad`` : result signed, operands signed
  /// * ``llvm.genx.sumad`` : result signed, operands unsigned
  /// * ``llvm.genx.usmad`` : result unsigned, operands signed
  /// * ``llvm.genx.uumad`` : result unsigned, operands unsigned
  ///
  /// result := arg0 * arg1 + arg2
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///                 vector width
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as result
  ///
  def int_genx_ssmad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_sumad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_usmad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_uumad : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*mad.sat`` : mad instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmad.sat`` : result signed, operands signed
  /// * ``llvm.genx.sumad.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usmad.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uumad.sat`` : result unsigned, operands unsigned
  ///
  /// result := sat(arg0 * arg1 + arg2)
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///                 vector width
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  /// * arg2: third input, same type as result
  ///
  def int_genx_ssmad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_sumad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_usmad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_uumad_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*max`` : max instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smax`` : result and operands signed
  /// * ``llvm.genx.umax`` : result and operands unsigned
  /// * ``llvm.genx.fmax`` : result and operands float
  ///
  /// * arg0: first input, any scalar/vector integer/float type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar, vector integer/float type with same
  ///               vector width
  ///
  /// There is no need for a saturating variant of this intrinsic.
  /// Because max cannot overflow, a saturating max can be represented
  /// by this non-saturating max followed by the applicable one of the
  /// saturating trunc intrinsics.
  ///
  def int_genx_smax : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umax : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_fmax : Intrinsic<[llvm_anyfloat_ty],
                [llvm_anyfloat_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*min`` : min instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smin`` : result and operands signed
  /// * ``llvm.genx.umin`` : result and operands unsigned
  /// * ``llvm.genx.fmin`` : result and operands float
  ///
  /// * arg0: first input, any scalar/vector integer/float type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer/float type with same
  ///               vector width
  ///
  /// There is no need for a saturating variant of this intrinsic.
  /// Because min cannot overflow, a saturating min can be represented
  /// by this non-saturating min followed by the applicable one of the
  /// saturating trunc intrinsics.
  ///
  def int_genx_smin : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umin : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_fmin : Intrinsic<[llvm_anyfloat_ty],
                [llvm_anyfloat_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// mod
  /// ^^^
  /// No intrinsic needed as the LLVM IR SRem, URem and FRem instructions
  /// cover vISA functionality
  ///

  /// mul
  /// ^^^
  /// Still need non-saaturating mul intrinsic as def-hoist/copy-prop in jitter
  /// cannot fully remove the trunc/zext/sext on each operand.
  ///
  /// ``llvm.genx.*mul`` : mul instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmul`` : result signed, operands signed, signed
  /// * ``llvm.genx.sumul`` : result signed, operands signed, unsigned
  /// * ``llvm.genx.usmul`` : result signed, operands unsigned, signed
  /// * ``llvm.genx.uumul`` : result signed, operands unsigned, unsigned
  ///
  /// result := arg0 * arg1
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///                 vector width
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  def int_genx_ssmul : Intrinsic<[llvm_anyint_ty],
                                 [llvm_anyint_ty, LLVMMatchType<1>],
                                 [IntrNoMem]>;
  def int_genx_sumul : Intrinsic<[llvm_anyint_ty],
                                 [llvm_anyint_ty, LLVMMatchType<1>],
                                 [IntrNoMem]>;
  def int_genx_usmul : Intrinsic<[llvm_anyint_ty],
                                 [llvm_anyint_ty, LLVMMatchType<1>],
                                 [IntrNoMem]>;
  def int_genx_uumul : Intrinsic<[llvm_anyint_ty],
                                 [llvm_anyint_ty, LLVMMatchType<1>],
                                 [IntrNoMem]>;

  /// ``llvm.genx.*mul.sat`` : mul instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssmul.sat`` : result signed, operands signed
  /// * ``llvm.genx.sumul.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usmul.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uumul.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type (not i64)
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type with same
  ///               vector width, even i64
  ///
  /// For an fp mul, use the LLVM IR FMul instruction, followed by
  /// llvm.genx.sat if saturation is required.
  ///
  def int_genx_ssmul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sumul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usmul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uumul_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*mulh`` : mulh instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.smulh`` : signed
  /// * ``llvm.genx.umulh`` : unsigned
  ///
  /// * arg0: first input, any scalar/vector i32 type
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, same type as arg0
  ///
  def int_genx_smulh : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_umulh : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// not
  /// ^^^
  /// Intrinsic not needed; use LLVM IR Xor instruction with -1
  ///

  /// or
  /// ^^
  /// Intrinsic not needed; use LLVM IR Or instruction
  ///

  /// ``llvm.genx.pln`` : plane equation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input value, vector float with exactly 4 elements
  /// * arg1: second input value, vector float with a multiple of 16 elements
  ///
  /// * Return value: result, vector float with half as many elements as arg1
  ///
  def int_genx_pln : Intrinsic<[llvm_anyfloat_ty],
                    [llvm_v4f32_ty, llvm_anyfloat_ty], [IntrNoMem]>;

  /// ``llvm.genx.pow`` : power
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: first input, any scalar/vector half/float type
  /// * arg1: second input, same type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_pow : Intrinsic<[llvm_anyfloat_ty],
              [LLVMMatchType<0>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndd`` : round down
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndd :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rnde`` : round to even
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rnde :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndu`` : round up
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndu :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rndz`` : round to zero
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rndz :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.rsqrt`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_rsqrt :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*sad2`` : two-wide sum of absolute differences
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssad2`` : signed argument and result
  /// * ``llvm.genx.usad2`` : unsigned argument and result
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  ///
  /// * Return value: result, vector of i16 of same vector width
  ///
  def int_genx_ssad2 : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usad2 : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*sad2add`` : two-wide sum of absolute differences and add
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sssad2add`` : signed result and args 
  /// * ``llvm.genx.uusad2add`` : unsigned result and args 
  /// * ``llvm.genx.ussad2add`` : unsigned result and signed args 
  /// * ``llvm.genx.susad2add`` : signed result and unsigned args 
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  /// * arg2: third input, vector of i16 of same vector width
  ///
  /// * Return value: result, same type as arg2
  ///
  def int_genx_sssad2add : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_uusad2add : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ussad2add : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_susad2add : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*sad2add.sat`` : two-wide sum of absolute differences and add, saturated
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.sssad2add.sat`` : signed result and args 
  /// * ``llvm.genx.uusad2add.sat`` : unsigned result and args 
  /// * ``llvm.genx.ussad2add.sat`` : unsigned result and signed args 
  /// * ``llvm.genx.susad2add.sat`` : signed result and unsigned args 
  ///
  /// * arg0: first input, vector of i8, multiple of 2 wide
  /// * arg1: second input, same type
  /// * arg2: third input, vector of i16 of same vector width
  ///
  /// * Return value: result, same type as arg2
  ///
  def int_genx_sssad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_uusad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_ussad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_susad2add_sat : Intrinsic<[llvm_anyint_ty], [llvm_anyint_ty,
              LLVMMatchType<1>, LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.*shl`` : shl instruction, no saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssshl`` : result signed, operands signed
  /// * ``llvm.genx.sushl`` : result signed, operands unsigned
  /// * ``llvm.genx.usshl`` : result unsigned, operands signed
  /// * ``llvm.genx.uushl`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar or vector integer type with same
  ///               vector width, even i64
  ///
  def int_genx_ssshl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sushl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usshl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uushl : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// ``llvm.genx.*shl.sat`` : shl instruction with saturation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.ssshl.sat`` : result signed, operands signed
  /// * ``llvm.genx.sushl.sat`` : result signed, operands unsigned
  /// * ``llvm.genx.usshl.sat`` : result unsigned, operands signed
  /// * ``llvm.genx.uushl.sat`` : result unsigned, operands unsigned
  ///
  /// * arg0: first input, any scalar/vector integer type, even i64
  /// * arg1: second input, same type as arg0
  ///
  /// * Return value: result, any scalar/vector integer type with same
  ///               vector width, even i64
  ///
  def int_genx_ssshl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_sushl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_usshl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;
  def int_genx_uushl_sat : Intrinsic<[llvm_anyint_ty],
                [llvm_anyint_ty, LLVMMatchType<1>], [IntrNoMem]>;

  /// shr
  /// ^^^
  /// Intrinsic is not needed. Because shr cannot overflow, an shr that
  /// saturates with a smaller result type than the execution type can be
  /// represented by an LLVM IR Shr instruction then an llvm.genx.sstrunc.sat.
  ///

  /// ``llvm.genx.sin`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sin :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.sqrt`` : reciprocal square root
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector half/float type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_sqrt :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  /// ``llvm.genx.ieee.sqrt`` : reciprocal square root, IEEE variant
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: input value, any scalar/vector float/double type
  ///
  /// * Return value: result, same type
  ///
  def int_genx_ieee_sqrt :
              Intrinsic<[llvm_anyfloat_ty], [LLVMMatchType<0>], [IntrNoMem]>;


  /// subb
  /// ^^^^
  /// No intrinsic for subb as it has two results.
  ///

  /// xor
  /// ^^^
  /// Intrinsic not needed; use LLVM IR Xor instruction
  ///

  // ---------------------------------
  /// vISA reserved register intrinsics
  /// ---------------------------------

  /// ``llvm.genx.thread.*`` : read thread ID register
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.thread.x`` : read vISA v1 (%thread_x)
  /// * ``llvm.genx.thread.y`` : read vISA v2 (%thread_y)
  ///
  /// * Return value:  i16 the value read
  ///
  def int_genx_thread_x : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;
  def int_genx_thread_y : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.group.id.*`` : read group ID register
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// ``llvm.genx.group.id.x`` : read vISA v7 (%group_id_x)
  /// ``llvm.genx.group.id.y`` : read vISA v8 (%group_id_y)
  /// ``llvm.genx.group.id.z`` : read vISA v23 (%group_id_z)
  ///
  /// * Return value:  i32 the value read
  ///
  def int_genx_group_id_x : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_group_id_y : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;
  def int_genx_group_id_z : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.timestamp`` : read vISA v11 (%timestamp)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * Return value:  vxi32 the value read
  ///
  /// The vector width must be power of 2 and no larger than 4.
  ///
  def int_genx_timestamp : Intrinsic<[llvm_anyint_ty], [], []>;

  /// ``llvm.genx.r0`` : read vISA v12 (%r0)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * Return value:  vxi32 or i32 the value read
  ///
  /// The vector width must be power of 2 and no larger than 8.
  ///
  def int_genx_r0 : Intrinsic<[llvm_anyint_ty], [], [IntrReadMem]>;

  /// ``llvm.genx.sr0`` : read vISA v18 (%sr0)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * Return value:  vxi32 the value read
  ///
  /// The vector width must be 4
  ///
  ///
  def int_genx_sr0 : Intrinsic<[llvm_anyint_ty], [], [IntrReadMem]>;

  /// ``llvm.genx.get.color`` : read color value of the thread origin
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// Return Value: i16 the value read
  ///
  /// This may not be the most appropriate way to access this value,
  /// but is a stop-gap solution.
  ///
  def int_genx_get_color : Intrinsic<[llvm_i16_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.get.hwid`` : read hw_id value
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// Return Value: i32 the value read
  ///
  def int_genx_get_hwid: Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  /// ``llvm.genx.set.pause`` : set the pause register (v11.4)
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// arg0: length of pause 10 bits (0-4 must be 0)
  ///
  /// Return Value: none
  /// 
  ///
  /// Set the pause value - this pauses instruction issue until the value has been
  /// decremented to 0 (decrements every 32 clocks)
  ///
  /// We set this intrinsic to have side-effects (last field empty) to stop it being removed as it
  /// otherwise looks dead
  def int_genx_set_pause : Intrinsic<[], [llvm_i16_ty], []>;

  /// ``llvm.genx.dummy.mov`` : insert a dummy mov to v0
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// arg0: a value that we want to mov to v0 (usually to trigger a scoreboard dependency)
  ///
  /// Return Value: none
  /// 
  ///
  /// This is primarily used to set up scoreboard dependencies. If a value is mov'ed to v0 then it
  /// will trigger a scoreboard dependency check.
  /// As a word (16 bits) is usually the basic type of value that is worked with, you only need to
  /// dummy mov one of these from any payload to correctly trigger the dependency
  ///
  /// We set this intrinsic to have side-effects (last field empty) to stop it being removed as it
  /// otherwise looks dead and also to prevent any kind of code motion optimisation
  def int_genx_dummy_mov : Intrinsic<[], [llvm_i16_ty], []>;

  // --------------------------
  /// Shared function intrinsics
  /// --------------------------
  /// These are in the order they appear in the vISA spec, not in
  /// alphabetical order.
  ///

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.add`` : vISA DWORD_ATOMIC ADD instruction
  /// * ``llvm.genx.dword.atomic.sub`` : vISA DWORD_ATOMIC SUB instruction
  /// * ``llvm.genx.dword.atomic.min`` : vISA DWORD_ATOMIC MIN instruction
  /// * ``llvm.genx.dword.atomic.max`` : vISA DWORD_ATOMIC MAX instruction
  /// * ``llvm.genx.dword.atomic.xchg`` : vISA DWORD_ATOMIC XCHG instruction
  /// * ``llvm.genx.dword.atomic.and`` : vISA DWORD_ATOMIC AND instruction
  /// * ``llvm.genx.dword.atomic.or`` : vISA DWORD_ATOMIC OR instruction
  /// * ``llvm.genx.dword.atomic.xor`` : vISA DWORD_ATOMIC XOR instruction
  /// * ``llvm.genx.dword.atomic.imin`` : vISA DWORD_ATOMIC IMIN instruction
  /// * ``llvm.genx.dword.atomic.imax`` : vISA DWORD_ATOMIC IMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 src
  /// * arg4: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with fmin/fmax operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.fmin`` : vISA DWORD_ATOMIC FMIN instruction
  /// * ``llvm.genx.dword.atomic.fmax`` : vISA DWORD_ATOMIC FMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXfloat src
  /// * arg4: vXfloat original value of the register that the data is read into
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_fmin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_fmax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.*`` : dword atomic with inc/dec operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.dword.atomic.inc`` : vISA DWORD_ATOMIC INC instruction
  /// * ``llvm.genx.dword.atomic.dec`` : vISA DWORD_ATOMIC DEC instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_dword_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.cmpxchg`` : vISA DWORD_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXi32 src0
  /// * arg4: vXi32 src1
  /// * arg5: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.dword.atomic.fcmpwr`` : vISA DWORD_ATOMIC FCMPWR instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 element offset in bytes
  /// * arg3: vXfloat src0
  /// * arg4: vXfloat src1
  /// * arg5: vXfloat original value of the register that the data is read into
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 1, 8 or 16.
  ///
  def int_genx_dword_atomic_fcmpwr : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.typed.atomic.*`` : atomic typed with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.typed.atomic.add`` : vISA TYPED_ATOMIC ADD instruction
  /// * ``llvm.genx.typed.atomic.sub`` : vISA TYPED_ATOMIC SUB instruction
  /// * ``llvm.genx.typed.atomic.min`` : vISA TYPED_ATOMIC MIN instruction
  /// * ``llvm.genx.typed.atomic.max`` : vISA TYPED_ATOMIC MAX instruction
  /// * ``llvm.genx.typed.atomic.xchg`` : vISA TYPED_ATOMIC XCHG instruction
  /// * ``llvm.genx.typed.atomic.and`` : vISA TYPED_ATOMIC AND instruction
  /// * ``llvm.genx.typed.atomic.or`` : vISA TYPED_ATOMIC OR instruction
  /// * ``llvm.genx.typed.atomic.xor`` : vISA TYPED_ATOMIC XOR instruction
  /// * ``llvm.genx.typed.atomic.imin`` : vISA TYPED_ATOMIC IMIN instruction
  /// * ``llvm.genx.typed.atomic.imax`` : vISA TYPED_ATOMIC IMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXT src
  /// * arg3: vXi32 u
  /// * arg4: vXi32 v - can be a constant 0 and becomes undef in lowering
  /// * arg5: vXi32 r - can be a constant 0 and becomes undef in lowering
  /// * arg6: vXi32 LOD - can be constant 0 and becomes undef in lowering
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width (which in reality must be 8)
  ///
  def int_genx_typed_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;

  /// ``llvm.genx.typed.atomic.*`` : atomic typed with fmin/fmax operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.typed.atomic.fmin`` : vISA TYPED_ATOMIC FMIN instruction
  /// * ``llvm.genx.typed.atomic.fmax`` : vISA TYPED_ATOMIC FMAX instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXfloat src
  /// * arg3: vXi32 u
  /// * arg4: vXi32 v - can be a constant 0 and becomes undef in lowering
  /// * arg5: vXi32 r - can be a constant 0 and becomes undef in lowering
  /// * arg6: vXi32 LOD - can be a constant 0 and becomes undef in lowering
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width (which in reality must be 8)
  ///
  def int_genx_typed_atomic_fmin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_fmax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;

  /// ``llvm.genx.typed.atomic.*`` : atomic typed with inc/dec operation
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.typed.atomic.inc`` : vISA TYPED_ATOMIC INC instruction
  /// * ``llvm.genx.typed.atomic.dec`` : vISA TYPED_ATOMIC DEC instruction
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXi32 u
  /// * arg3: vXi32 v - can be a constant 0 and becomes undef in lowering
  /// * arg4: vXi32 r - can be a constant 0 and becomes undef in lowering
  /// * arg5: vXi32 LOD - can be a constant 0 and becomes undef in lowering
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width (which in reality must be 8)
  ///
  def int_genx_typed_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;
  def int_genx_typed_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;

  /// ``llvm.genx.typed.atomic.cmpxchg`` : vISA TYPED_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXT src0
  /// * arg3: vXT src1
  /// * arg3: vXi32 u
  /// * arg4: vXi32 v - can be a constant 0 and becomes undef in lowering
  /// * arg5: vXi32 r - can be a constant 0 and becomes undef in lowering
  /// * arg6: vXi32 LOD - can be a constant 0 and becomes undef in lowering
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width (which in reality must be 8)
  ///
  def int_genx_typed_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>, LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;

  /// ``llvm.genx.typed.atomic.fcmpwr`` : vISA TYPED_ATOMIC FCMPWR instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 surface index
  /// * arg2: vXfloat src0
  /// * arg2: vXfloat src1
  /// * arg3: vXi32 u
  /// * arg4: vXi32 v - can be a constant 0 and becomes undef in lowering
  /// * arg5: vXi32 r - can be a constant 0 and becomes undef in lowering
  /// * arg6: vXi32 LOD - can be a constant 0 and becomes undef in lowering
  ///
  /// * Return value: vXfloat the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width (which in reality must be 8)
  ///
  def int_genx_typed_atomic_fcmpwr : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, LLVMMatchType<0>,LLVMMatchType<0>,
	llvm_anyint_ty, LLVMMatchType<2>, LLVMMatchType<2>, LLVMMatchType<2>], []>;

  /// ``llvm.genx.gather.orig`` : vISA GATHER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: Elt_size inferred from argument type
  /// * arg1: i32 is_modified, constant
  /// * (Num_elts inferred from data type)
  /// * arg2: i32 surface index
  /// * arg3: i32 global offset in elements
  /// * arg4: vXi32 element offset in elements
  /// * arg5: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the return value is the number of elements to read,
  /// which must be 1, 8 or 16.
  ///
  /// The element offset arg must have the same vector width.
  ///
  // (Silly name because of LLVM prefix bug.)
  def int_genx_gather_orig : Intrinsic<[llvm_anyvector_ty], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>],
    [IntrReadMem]>;

  /// ``llvm.genx.gather.scaled`` : vISA GATHER_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  ///       (block size MBZ, means 1 byte)
  /// * arg1: i32 log2 num blocks, constant (0/1/2 for num blocks 1/2/4)
  /// * arg2: i16 scale, constant
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes (X = 8 or 16)
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  ///
  /// The predicate arg must have the same vector width.
  ///
  /// The block size must be 1 byte.
  ///
  /// Only T0 (SLM) and T5 (stateless) are supported.
  ///
  /// The old value of the data read (the return value) must have UD, D or
  /// F type. For 1 and 2 byte (1 x num blocks) reads the upper bytes have
  /// undefined values in the returned value.
  ///
  /// This instruction is available for SKL+ in general and it works for pre-SKL
  /// only when scale is 0.
  ///
  def int_genx_gather_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4.orig`` : vISA GATHER4 instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 is_modified, constant
  /// * arg2: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in i32s
  /// * arg5: vXi32 element offset in i32s
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  // (Silly name because of LLVM prefix bug.)
  def int_genx_gather4_orig : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
    LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4.scaled`` : vISA GATHER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg2: i16 scale, constant
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must have the same vector width.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_gather4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.gather4.typed`` : vISA GATHER4_TYPED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg2: i32 surface index
  /// * arg3: vXi32 U pixel address
  /// * arg4: vXi32 V pixel address
  /// * arg5: vXi32 R pixel address
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal and are the number of elements to read, which
  /// must be 8 or 16. (16 is split into 2x 8 by the GenX backend.)
  /// The predicate arg must have the same vector width.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels to read.
  /// The number of 1 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element. Mask "0000" is not allowed.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_gather4_typed : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyvector_ty, llvm_i32_ty, llvm_anyvector_ty, LLVMMatchType<2>,
    LLVMMatchType<2>, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.media.ld`` : vISA MEDIA_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 modifiers, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 plane, constant
  /// * arg3: i32 block width in bytes, constant
  /// * (block height inferred from return type size and block width)
  /// * arg4: i32 x byte offset
  /// * arg5: i32 y byte offset
  ///
  /// * Return value: the data read.
  ///
  /// The number of bytes taken by a row in the return value, the "rounded
  /// block width", is the block width rounded up to the next power of two
  /// no less than 4. The size of the return type must be a multiple of
  /// this rounded block width, and the multiplier is the block height.
  ///
  /// The block width has a maximum of 32 (64 on BDW+). The maxmimum byte
  /// size of the return type is 256.
  ///
  def int_genx_media_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.media.st`` : vISA MEDIA_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 modifiers, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 plane, constant
  /// * arg3: i32 block width in bytes, constant
  /// * (block height inferred from data type size and block width)
  /// * arg4: i32 x byte offset
  /// * arg5: i32 y byte offset
  /// * arg6: data to write
  ///
  /// The number of bytes taken by a row in the return value, the "rounded
  /// block width", is the block width rounded up to the next power of two
  /// no less than 4. The size of the data to write type must be a multiple of
  /// this rounded block width, and the multiplier is the block height.
  ///
  /// The block width has a maximum of 32 (64 on BDW+). The maxmimum byte
  /// size of the data to write is 256.
  ///
  def int_genx_media_st : Intrinsic<[], [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.oword.ld*`` : oword load instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.oword.ld`` : vISA OWORD_LD instruction
  /// * ``llvm.genx.oword.ld.unaligned`` : vISA OWORD_LD_UNALIGNED instruction
  ///
  /// * (log2 number of owords inferred from return type)
  /// * arg0: i32 is_modified, constant
  /// * arg1: i32 surface index
  /// * arg2: i32 offset (in owords for .ld / in bytes for .ld.unaligned)
  ///
  /// * Return value: the data read.
  ///
  /// The byte size of the return type must be 16, 32, 64, or 128.
  ///
  def int_genx_oword_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;
  def int_genx_oword_ld_unaligned : Intrinsic<[llvm_anyvector_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.oword.st`` : vISA OWORD_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (log2 number of owords inferred from return type)
  /// * arg0: i32 surface index
  /// * arg1: i32 offset (in owords)
  /// * arg2: data to write
  ///
  /// The byte size of the data to write must be 16, 32, 64, or 128.
  ///
  def int_genx_oword_st :
    Intrinsic<[], [llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter.orig`` : vISA SCATTER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: Elt_size from argument element type
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in elements
  /// * arg3: vXi32 element offset in elements
  /// * arg4: the data to write. The first <num_elts> elements will be used.
  ///
  /// The operand must have one of UD, D, F type; for 1 and 2 byte accesses the upper
  /// bits will be ignored.
  ///
  /// The vector width of the data to write is the number of elements to write,
  /// which must be 1, 8 or 16.
  /// The element offset arg must have the same vector width.
  ///
  // (Silly name because of LLVM prefix bug.)
  def int_genx_scatter_orig : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter.scaled`` : vISA SCATTER_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  ///       (block size MBZ, means 1 byte)
  /// * arg1: i32 log2 num blocks, constant (0/1/2 for num blocks 1/2/4)
  /// * arg2: i16 scale, constant
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset (X = 8 or 16)
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// write, which must be 8 or 16.
  ///
  /// The predicate arg must have the same vector width.
  ///
  /// The block size must be 1 byte.
  ///
  /// Only T0 (SLM) and T5 (stateless) are supported.
  ///
  /// The data type to write must have UD, D or F type. For 1 and 2 byte (1 x num
  /// blocks) accesses the upper bytes will be ignored.
  ///
  /// This instruction is available for SKL+ in general and it works for pre-SKL
  /// only when scale is 0.
  ///
  def int_genx_scatter_scaled : Intrinsic<[], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
    llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter4.orig`` : vISA SCATTER4 instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg2: i32 surface index
  /// * arg3: i32 global offset in i32s
  /// * arg4: vXi32 element offset in i32s
  /// * arg5: the data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to write.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The vector width of the data to write arg must be the number of elements
  /// times the number of channels to write per element.
  /// The element type of the data to write must be i32 or float.
  ///
  // (Silly name because of LLVM prefix bug.)
  def int_genx_scatter4_orig : Intrinsic<[], [llvm_i32_ty, llvm_anyvector_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.scatter4.scaled`` : vISA SCATTER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (Exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg2: i16 scale, constant
  /// * arg3: i32 surface index
  /// * arg4: i32 global offset in bytes
  /// * arg5: vXi32 element offset in bytes
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// write, which must be 8 or 16.
  /// The predicate arg must have the same vector width.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The channels to write must be contiguous and starting at channel 0.
  /// The vector width of the data to write must be the number of elements
  /// times the number of channels to write per element.
  /// The element type of the data to write must be i32 or float.
  ///
  def int_genx_scatter4_scaled : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_i16_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty, llvm_anyvector_ty],
    []>;

  /// ``llvm.genx.scatter4.typed`` : vISA SCATTER4_TYPED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: vXi1 predicate (Num_elts inferred from U pixel address type)
  /// * arg2: i32 surface index
  /// * arg3: v8Xi32 U pixel address
  /// * arg4: v8Xi32 V pixel address
  /// * arg5: v8Xi32 R pixel address
  /// * arg6: data to write
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal and are the number of elements to write, which
  /// must be 8 or 16. (16 is split into 2x 8 by the GenX backend.)
  /// The predicate arg must have the same vector width.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels to write.
  /// The number of 1 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element. Mask "0000" is not allowed.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the source value must be i32 or float.
  ///
  def int_genx_scatter4_typed : Intrinsic<[], [llvm_i32_ty, llvm_anyvector_ty,
    llvm_i32_ty, llvm_anyvector_ty, LLVMMatchType<1>, LLVMMatchType<1>,
    llvm_anyvector_ty], []>;

  /// ``llvm.genx.transpose.ld`` : vISA TRANSPOSE_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: i32 log2 block width in i32s, constant (0-3)
  /// * (log2 block height inferred from block width and data type, 0-3)
  /// * arg2: i32 X offset
  /// * arg3: i32 Y offset
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the return value is the number of elements to read.
  /// This must be a multiple of the block width. The block height is then
  /// inferred from those values.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_transpose_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.untyped.atomic.*`` : vISA UNTYPED_ATOMIC with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.untyped.atomic.add`` : vISA UNTYPED_ATOMIC ADD instruction
  /// * ``llvm.genx.untyped.atomic.sub`` : vISA UNTYPED_ATOMIC SUB instruction
  /// * ``llvm.genx.untyped.atomic.min`` : vISA UNTYPED_ATOMIC MIN instruction
  /// * ``llvm.genx.untyped.atomic.max`` : vISA UNTYPED_ATOMIC MAX instruction
  /// * ``llvm.genx.untyped.atomic.xchg`` : vISA UNTYPED_ATOMIC XCHG instruction
  /// * ``llvm.genx.untyped.atomic.and`` : vISA UNTYPED_ATOMIC AND instruction
  /// * ``llvm.genx.untyped.atomic.or`` : vISA UNTYPED_ATOMIC OR instruction
  /// * ``llvm.genx.untyped.atomic.xor`` : vISA UNTYPED_ATOMIC XOR instruction
  /// * ``llvm.genx.untyped.atomic.imin`` : vISA UNTYPED_ATOMIC IMIN instruction
  /// * ``llvm.genx.untyped.atomic.imax`` : vISA UNTYPED_ATOMIC IMAX instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 src
  /// * arg5: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  //same vector / width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.untyped.atomic.*`` : vISA UNTYPED_ATOMIC with inc/dec
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.untyped.atomic.inc`` : vISA UNTYPED_ATOMIC INC instruction
  /// * ``llvm.genx.untyped.atomic.dec`` : vISA UNTYPED_ATOMIC DEC instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset and the return value must have the same vector
  /// width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;
  def int_genx_untyped_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.untyped.atomic.cmpxchg`` : vISA UNTYPED_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: i32 surface index
  /// * arg2: i32 global offset in i32s
  /// * arg3: vXi32 element offset in i32s
  /// * arg4: vXi32 src0
  /// * arg5: vXi32 src1
  /// * arg6: vXi32 original value of the register that the data is read into
  ///
  /// * Return value: vXi32 the old value read
  ///
  /// Predicate, element offset, src0, src1, and the return value must all have
  /// the same vector width, which must be 8 or 16.
  ///
  def int_genx_untyped_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, LLVMMatchType<0>,
    LLVMMatchType<0>, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.block.ld*`` : vISA SVM BLOCK_LD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.block.ld`` : vISA SVM BLOCK_LD instruction with oword alignment
  /// * ``llvm.genx.svm.block.ld.unaligned`` : vISA SVM BLOCK_LD instruction with
  ///   dword alignment
  ///
  /// * (log2 number of oword inferred from data type)
  /// * arg0: i64 address
  ///
  /// * Return value: data read
  ///
  /// The data read must have a size that is a power of two from 16 to 128
  /// bytes.
  ///
  def int_genx_svm_block_ld : Intrinsic<[llvm_anyvector_ty], [llvm_i64_ty],
    [IntrReadMem]>;
  def int_genx_svm_block_ld_unaligned : Intrinsic<[llvm_anyvector_ty],
    [llvm_i64_ty], [IntrReadMem]>;

  /// ``llvm.genx.svm.block.st`` : vISA SVM BLOCK_ST instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (log2 number of oword inferred from data type)
  /// * arg0: i64 address
  /// * arg1: data to write
  ///
  /// The data to write must have a size that is a power of two from 16 to 128
  /// bytes.
  ///
  def int_genx_svm_block_st : Intrinsic<[], [llvm_i64_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.svm.gather`` : vISA SVM GATHER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec size inferred from address vector width)
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * (block size inferred from data element type)
  /// * arg1: i32 log2 num blocks, constant (0/1/2/3 for num blocks 1/2/4/8)
  /// * arg2: vXi64 address (X = 8 or 16)
  /// * arg3: old value of the data read
  /// 
  /// * Return value: data read
  ///
  /// The return value element type is i8 for block size 1, i32/float for
  /// block size 4, or i64/double for block size 8.
  /// The return value vector width is the address vector width times
  /// number of blocks (rounded up to 4 if block size is 1).
  ///
  def int_genx_svm_gather : Intrinsic<[llvm_anyvector_ty], [llvm_anyvector_ty,
    llvm_i32_ty, llvm_anyint_ty, LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.svm.gather4.scaled`` : vISA SVM GATHER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg3: i16 scale, constant
  /// * arg4: i64 global address in bytes
  /// * arg5: vXi64 element offset in bytes
  /// * arg6: old value of the data read
  ///
  /// * Return value: the data read
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  /// The vector width of the return value must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_svm_gather4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i64_ty, llvm_anyint_ty,
    LLVMMatchType<0>], [IntrReadMem]>;

  /// ``llvm.genx.svm.scatter`` : vISA SVM SCATTER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec size inferred from address vector width)
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * (block size inferred from data element type)
  /// * arg1: i32 log2 num blocks, constant (0/1/2/3 for num blocks 1/2/4/8)
  /// * arg2: vXi64 address (X = 8 or 16)
  /// * arg3: data to write
  ///
  /// The data to write element type is i8 for block size 1, i32/float for
  /// block size 4, or i64/double for block size 8.
  /// The data vector width is the address vector width times
  /// number of blocks (rounded up to 4 if block size is 1).
  ///
  def int_genx_svm_scatter : Intrinsic<[], [llvm_anyvector_ty, llvm_i32_ty,
    llvm_anyint_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.svm.scatter4.scaled`` : vISA SVM SCATTER4_SCALED instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * (exec_size inferred from element offset type)
  /// * arg0: vXi1 predicate
  /// * arg1: i32 channel mask, constant
  /// * arg3: i16 scale, constant
  /// * arg4: i64 global address in bytes
  /// * arg5: vXi64 element offset in bytes
  /// * arg6: data to write
  ///
  /// The vector width of the element offset arg is the number of elements to
  /// read, which must be 8 or 16.
  /// The predicate arg must either have the same vector width, or be a scalar
  /// i1 constant with value 1.
  /// The instruction writes up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to write per element.
  /// The vector width of the data to write arg must be the number of elements
  /// times the number of channels to read per element.
  /// The element type of the data to write arg must be i32 or float.
  ///
  def int_genx_svm_scatter4_scaled : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i64_ty, llvm_anyint_ty],
    []>;

  /// ``llvm.genx.svm.atomic.*`` : vISA SVM_ATOMIC with binary operator
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.atomic.add`` : vISA SVM_ATOMIC ADD instruction
  /// * ``llvm.genx.svm.atomic.sub`` : vISA SVM_ATOMIC SUB instruction
  /// * ``llvm.genx.svm.atomic.min`` : vISA SVM_ATOMIC MIN instruction
  /// * ``llvm.genx.svm.atomic.max`` : vISA SVM_ATOMIC MAX instruction
  /// * ``llvm.genx.svm.atomic.xchg`` : vISA SVM_ATOMIC XCHG instruction
  /// * ``llvm.genx.svm.atomic.and`` : vISA SVM_ATOMIC AND instruction
  /// * ``llvm.genx.svm.atomic.or`` : vISA SVM_ATOMIC OR instruction
  /// * ``llvm.genx.svm.atomic.xor`` : vISA SVM_ATOMIC XOR instruction
  /// * ``llvm.genx.svm.atomic.imin`` : vISA SVM_ATOMIC IMIN instruction
  /// * ``llvm.genx.svm.atomic.imax`` : vISA SVM_ATOMIC IMAX instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src
  /// * arg3: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset, src, and the return value must all have the
  /// same vector width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_add : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_sub : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_min : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_max : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_xchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_and : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_or : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_xor : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_imin : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_imax : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.atomic.*`` : vISA SVM_ATOMIC with inc/dec
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  /// * ``llvm.genx.svm.atomic.inc`` : vISA SVM_ATOMIC INC instruction
  /// * ``llvm.genx.svm.atomic.dec`` : vISA SVM_ATOMIC DEC instruction
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from this arg)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src
  /// * arg3: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset and the return value must have the same vector
  /// width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_inc : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>], []>;
  def int_genx_svm_atomic_dec : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>], []>;

  /// ``llvm.genx.svm.atomic.cmpxchg`` : vISA SVM_ATOMIC CMPXCHG instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: vXi1 predicate (Num_elts inferred from element offset type)
  /// * arg1: vXi64 element addresses in bytes
  /// * arg2: vXi32/vXi64 src0
  /// * arg3: vXi32/vXi64 src1
  /// * arg4: original value of the register that the data is read into
  ///
  /// * Return value: vXi32/vXi64 the old value read
  ///
  /// Predicate, element offset, src0, src1, and the return value must all have
  /// the same vector width, which must be 8 (BDW+) or 16 (SKL+).
  ///
  def int_genx_svm_atomic_cmpxchg : Intrinsic<[llvm_anyvector_ty],
    [llvm_anyvector_ty, llvm_anyint_ty, LLVMMatchType<0>, LLVMMatchType<0>,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.load`` : vISA LOAD (sampler load) instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant (simd_mode inferred from pixel address operands)
  /// * arg1: i32 surface index
  /// * arg2: vXi32 U pixel address
  /// * arg3: vXi32 V pixel address
  /// * arg4: vXi32 R pixel address
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal, and either 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  /// For SIMD8 pre-BDW, the vector width of the data read must be 32.
  /// For SIMD8 BDW+, or for SIMD16, the vector width of the data read must be
  /// the SIMD width times the number of enabled channels.
  ///
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_load : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_anyint_ty, LLVMMatchType<1>, LLVMMatchType<1>], [IntrReadMem]>;

  /// ``llvm.genx.sample`` : vISA SAMPLE instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant (simd_mode inferred from pixel address operands)
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: vXfloat U pixel address
  /// * arg4: vXfloat V pixel address
  /// * arg5: vXfloat R pixel address
  ///
  /// * Return value: the data read
  ///
  /// The vector widths of the U pixel address, V pixel address and R pixel
  /// address args must be equal, and either 8 or 16.
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  /// For SIMD8 pre-BDW, the vector width of the data read must be 32.
  /// For SIMD8 BDW+, or for SIMD16, the vector width of the data read must be
  /// the SIMD width times the number of enabled channels.
  ///
  /// The element type of the return value must be i32 or float.
  ///
  def int_genx_sample : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyfloat_ty, LLVMMatchType<1>,
    LLVMMatchType<1>], [IntrReadMem]>;

  /// ``llvm.genx.sample.unorm`` : vISA SAMPLE_UNORM instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: float U pixel address
  /// * arg4: float V pixel address
  /// * arg5: float DeltaU
  /// * arg6: float DeltaV
  ///
  /// * Return value: v8i16 the data read
  ///
  /// The instruction reads up to 4 channels per element, with the lowest 4
  /// bits of the channel mask arg giving the mask of channels _not_ to read.
  /// The number of 0 bits in that lower 4 bits of the channel mask arg is the
  /// number of channels to read per element.
  ///
  def int_genx_sample_unorm : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.3d.sample`` : vISA 3D_SAMPLE instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 opcode, constant
  /// * arg1: vXi1 predicate mask, used to determine execution size
  /// * arg2: i32 channel mask, constant
  /// * arg3: i16 aoffimmi
  /// * arg4: i32 sampler index
  /// * arg5: i32 surface index
  /// * argN: vXf or vXhf operand, for 6 <= N <= 20
  ///
  /// * Return value: the data read
  ///
  def int_genx_3d_sample : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty], [IntrReadMem]>;

  /// ``llvm.genx.3d.load`` : vISA 3D_LOAD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 opcode, constant
  /// * arg1: vXi1 predicate mask, used to determine execution size
  /// * arg2: i32 channel mask, constant
  /// * arg3: i16 aoffimmi
  /// * arg4: i32 surface index
  /// * argN: vXf or vXhf operand, for 5 <= N <= 19 
  ///
  /// * Return value: the data read
  ///
  def int_genx_3d_load : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyvector_ty, llvm_i32_ty, llvm_i16_ty, llvm_i32_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, llvm_anyvector_ty, llvm_anyvector_ty], [IntrReadMem]>;

  /// ``llvm.genx.avs`` : vISA AVS instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 channel mask, constant
  /// * arg1: i32 sampler index
  /// * arg2: i32 surface index
  /// * arg3: float U offset
  /// * arg4: float V offset
  /// * arg5: float deltaU
  /// * arg6: float deltaV
  /// * arg7: float u2d
  /// * arg8: i32 groupID
  /// * arg9: i32 verticalBlockNumber
  /// * arg10: i32 Output format control, constant
  /// * arg11: float v2d
  /// * arg12: i32 execMode, constant
  /// * arg13: i8 IEFBypass
  ///
  /// * Return value: the data read.
  ///
  /// The actual data returned is determined by a combination of <channel>,
  /// <cntrl>, <execMode>, as well as whether output shuffle is enabled in the
  /// sampler state.
  ///
  /// SIMD Control Flow: channel enable is ignored.
  ///
  def int_genx_avs : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty,
    llvm_float_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_float_ty,
    llvm_i32_ty, llvm_i8_ty], [IntrReadMem]>;

  /// ``llvm.genx.barrier`` : vISA BARRIER instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_barrier : Intrinsic<[],[],[]>;

  /// ``llvm.genx.cache.flush`` : vISA CACHE_FLUSH instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_cache_flush : Intrinsic<[],[],[]>;

  /// ``llvm.genx.fence`` : vISA FENCE instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i8 mask, constant
  ///
  def int_genx_fence : Intrinsic<[], [llvm_i8_ty], []>;

  /// ``llvm.genx.wait`` : vISA WAIT instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i8 thread mask
  ///
  def int_genx_wait : Intrinsic<[], [llvm_i8_ty], []>;

  /// ``llvm.genx.yield`` : vISA YIELD instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  def int_genx_yield : Intrinsic<[],[],[]>;

  /// ``llvm.genx.raw.send`` : vISA RAW_SEND instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  /// * (numdst inferred from dst size)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: old_dst
  ///
  /// * Return value: dst
  ///
  /// The SEND instruction has a field for the size of each of src
  /// and dst. These are inferred by rounding the size of each of src and
  /// dst up to the next whole GRF.
  ///
  /// If the send writes to the whole of dst, or the program does not care what
  /// was in those registers before, then set old_dst to UndefValue (of the same
  /// type as dst). If on the other hand the send is predicated and the program
  /// needs to see what was in the parts of destination registers not written
  /// by the send, then use old_dst as the "old value of destination registers"
  /// input.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_send : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyint_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty,
    LLVMMatchType<0>], []>;

  /// ``llvm.genx.raw.send.noresult`` : vISA RAW_SEND instruction with no result
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  ///       (numdst is 0)
  /// * arg3: i32 desc
  /// * arg4: src
  ///
  /// The SEND instruction has a field for the size of src. This is inferred by
  /// rounding the size of src up to the next whole GRF.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_send_noresult : Intrinsic<[], [llvm_i32_ty, llvm_anyint_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty], []>;

  /// ``llvm.genx.raw.sends`` : vISA RAW_SENDS instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor, constant
  /// * (numsrc inferred from src size)
  /// * (numsrc2 inferred from src2 size)
  /// * (numdst inferred from dst size)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: src2
  /// * arg6: old_dst
  ///
  /// * Return value: dst
  ///
  /// The SENDS instruction has a field for the size of each of src, src2
  /// and dst. These are inferred by rounding the size of each of src, src2 and
  /// dst up to the next whole GRF.
  ///
  /// If the send writes to the whole of dst, or the program does not care what
  /// was in those registers before, then set old_dst to UndefValue (of the same
  /// type as dst). If on the other hand the send is predicated and the program
  /// needs to see what was in the parts of destination registers not written
  /// by the send, then use old_dst as the "old value of destination registers"
  /// input.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_sends : Intrinsic<[llvm_anyvector_ty], [llvm_i32_ty,
    llvm_anyint_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty,
    llvm_anyvector_ty, LLVMMatchType<0>], []>;

  /// ``llvm.genx.raw.sends.noresult`` : vISA RAW_SENDS instruction with no result
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0  i32 modifier whether it is send or sendc, constant
  /// * (exec_size inferred from predicate vector width, defaulting to 16
  ///          if predicate is i1)
  /// * arg1: i1/vXi1 predicate
  /// * arg2: i32 extended message descriptor
  /// * (numsrc inferred from src size)
  /// * (numsrc2 inferred from src2 size)
  /// * (numdst is 0)
  /// * arg3: i32 desc
  /// * arg4: src
  /// * arg5: src2
  ///
  /// The SENDS instruction has a field for the size of each of src and src2.
  /// These are inferred by rounding the size of each of src and src2 up to the
  /// next whole GRF.
  ///
  /// The predicate must be constant i1 with value 1 for a message that is not
  /// predicatable. For a predicatable message, it must be a vector of i1 with
  /// width determining the execution size.
  ///
  def int_genx_raw_sends_noresult : Intrinsic<[], [llvm_i32_ty, llvm_anyint_ty,
    llvm_i32_ty, llvm_i32_ty, llvm_anyvector_ty, llvm_anyvector_ty], []>;

  /// ---------------------------
  /// Video Analytics Instrinsics
  /// ---------------------------
  ///
  /// ``llvm.genx.va.convolve2d`` vISA VA 2d Convolve instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 properties value specifying return data format and input region size, constant
  ///
  /// * Return value: v64i16 or v16i16 matrix, depending on properties value
  ///
  def int_genx_va_convolve2d : Intrinsic<[llvm_anyint_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.convolve2d`` vISA VA HDC 2d Convolve instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 properties value specifying return data format and input region size, constant
  /// * arg5: i32 destination surface
  /// * arg6: i16 destination surface x-offset
  /// * arg7: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_convolve2d : Intrinsic<[], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty, llvm_i32_ty, llvm_i16_ty, llvm_i16_ty],
    []>;

  /// ``llvm.genx.va.erode`` vISA VA Erode instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 properties value specifying return data format, constant
  ///
  /// * Return value: vXi32
  ///
  def int_genx_va_erode : Intrinsic<[llvm_anyint_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.erode`` vISA VA HDC Erode instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 destination surface
  /// * arg5: i16 destination surface x-offset
  /// * arg6: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_erode : Intrinsic<[], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty, llvm_i16_ty, llvm_i16_ty],
    []>;

  /// ``llvm.genx.va.dilate`` vISA VA Dilate instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 properties value specifying return data format, constant
  ///
  /// * Return value: vXi32
  ///
  def int_genx_va_dilate : Intrinsic<[llvm_anyint_ty], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.dilate`` vISA VA HDC Dilate instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 destination surface
  /// * arg5: i16 destination surface x-offset
  /// * arg6: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_dilate : Intrinsic<[], [llvm_i32_ty, llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty, llvm_i16_ty, llvm_i16_ty],
    []>;

  /// ``llvm.genx.va.minmax`` vISA MinMax instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: i32 enable specific minmax functionality
  ///
  /// * Return: v32i8 or v16i16 depending on the surface format
  ///
  def int_genx_va_minmax : Intrinsic<[llvm_anyint_ty], [llvm_i32_ty,
    llvm_float_ty, llvm_float_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.minmax.filter`` vISA MinMax Filter instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 specifies the size of the minmax value returned, constant
  /// * arg5: i32 specifies the return data format, constant
  /// * arg6: i32 enable specific minmax functionality
  ///
  /// * Return: vXi8 or vXi16 depending on return data size and format
  ///
  def int_genx_va_minmax_filter : Intrinsic<[llvm_anyint_ty], [llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.minmax.filter`` vISA HDC MinMax Filter instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 return data format, constant
  /// * arg5: i32 enable the specific minmax functionality, constant
  /// * arg6: i32 destination surface index
  /// * arg7: i16 destination surface x-offset
  /// * arg8: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_minmax_filter : Intrinsic<[], [llvm_i32_ty,
    llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty, llvm_i32_ty,
    llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.bool.centroid`` vISA Boolean Centroid instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: i8 vertical size
  /// * arg4: i8 horizontal size
  ///
  /// * Return: v16i8 or v16i16 depending on surface format
  ///
  def int_genx_va_bool_centroid : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i8_ty, llvm_i8_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.centroid`` vISA Centroid instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: i8 vertical size
  ///
  /// * Return: v32i32
  ///
  def int_genx_va_centroid : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i8_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.1d.convolve.horizontal`` vISA 1d convolve horizontal instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 mode, constant
  ///
  /// * Return: v16i16 or v64i16 depending on mode
  ///
  def int_genx_va_1d_convolve_horizontal : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.1d.convolve.horizontal`` vISA HDC 1d convolve horizontal instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 pixel size, constant
  /// * arg5: i32 destination surface index
  /// * arg6: i16 destination surface x-offset
  /// * arg7: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_1d_convolve_horizontal : Intrinsic<[],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty,
     llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.1d.convolve.vertical`` vISA 1d convolve vertical instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 mode, constant
  ///
  /// * Return: v16i16 or v64i16 depending on mode
  ///
  def int_genx_va_1d_convolve_vertical : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty],
    [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.1d.convolve.vertical`` vISA HDC 1d convolve vertical instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 pixel size, constant
  /// * arg5: i32 destination surface index
  /// * arg6: i16 destination surface x-offset
  /// * arg7: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_1d_convolve_vertical : Intrinsic<[],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty,
     llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.1pixel.convolve`` vISA 1 Pixel Convolve instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 mode, constant
  /// * arg5: v32i16 offsets
  ///
  /// * Return: v64i16 or v16i16 depending on mode.
  ///
  def int_genx_va_1pixel_convolve : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty,
     llvm_i32_ty, llvm_anyint_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.1pixel.convolve`` vISA HDC 1 Pixel Convolve instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  /// * arg4: i32 pixel size, constant
  /// * arg5: v32i16 offsets
  /// * arg6: i32 destination surface index
  /// * arg7: i16 destination surface x-offset
  /// * arg8: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_1pixel_convolve : Intrinsic<[],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty,
     llvm_i32_ty, llvm_anyint_ty, llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.1pixel.convolve.1x1mode`` vISA 1 Pixel Convolve (1x1 mode) instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 sampler index
  /// * arg1: i32 surface index
  /// * arg2: float normalized x-coordinate
  /// * arg3: float normalized y-coordinate
  ///
  /// * Return: v64i16 or v16i16 depending on mode.
  ///
  def int_genx_va_1pixel_convolve_1x1mode : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_i32_ty, llvm_float_ty, llvm_float_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.lbp.creation`` vISA LBP Creation instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: mode, constant
  ///
  /// * Return: v64i8 or v128i8 depending on mode
  ///
  def int_genx_va_lbp_creation : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.lbp.creation`` vISA HDC LBP Creation instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: mode, constant
  /// * arg4: i32 destination surface index
  /// * arg5: i16 destination surface x-offset
  /// * arg6: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_lbp_creation : Intrinsic<[],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i32_ty,
     llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.lbp.correlation`` vISA LBP Correlation instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: i16 horizontal disparity
  ///
  /// * Return: v64i8
  ///
  def int_genx_va_lbp_correlation : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i16_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.hdc.lbp.correlation`` vISA HDC LBP Correlation instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: i16 horizontal disparity
  /// * arg4: i32 destination surface index
  /// * arg5: i16 destination surface x-offset
  /// * arg6: i16 destination surface y-offset
  ///
  def int_genx_va_hdc_lbp_correlation : Intrinsic<[],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_i16_ty,
     llvm_i32_ty, llvm_i16_ty, llvm_i16_ty], []>;

  /// ``llvm.genx.va.correlation.search`` vISA Correlation Search instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i32 surface index
  /// * arg1: float normalized x-coordinate
  /// * arg2: float normalized y-coordinate
  /// * arg3: float normalized vertical origin
  /// * arg4: float normalized horizontal origin
  /// * arg5: i8 x-direction size
  /// * arg6: i8 y-direction size
  /// * arg7: i8 x-direction search size
  /// * arg8: i8 y-direction search size
  ///
  /// * Return: vXi32
  ///
  def int_genx_va_correlation_search : Intrinsic<[llvm_anyint_ty],
    [llvm_i32_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty, llvm_float_ty,
    llvm_i8_ty, llvm_i8_ty, llvm_i8_ty, llvm_i8_ty], [IntrReadMem]>;

  /// ``llvm.genx.va.flood.fill`` vISA Flood Fill instruction
  /// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  ///
  /// * arg0: i8 Is8Connect, constant (valid values 0 or 1).
  /// * arg1: v10i16 pixel mask horizontal direction
  /// * arg2: i16 pixel mask vertical direction left
  /// * arg3: i16 pixel mask vertical direction right
  /// * arg4: i16 loop count
  ///
  /// * Return: v8i16
  ///
  def int_genx_va_flood_fill : Intrinsic<[llvm_anyint_ty],
    [llvm_i8_ty, llvm_anyint_ty, llvm_i16_ty, llvm_i16_ty, llvm_i16_ty],
    [IntrReadMem]>;

  //--------------------------------------------------------------------
  // CM codegen internal intrinsics

  // ``llvm.genx.simdcf.predicate`` : simd cf predication marker intrinsic.
  //
  // * arg0: vector with any element type
  // * arg1: vector constant, same size as arg0
  //
  // * Return value: a vector composed of elements selected from arg0 or arg1
  //   according to the implied SIMD CF predication mask.
  //
  // This is generated by clang codegen in the implementation of the CM
  // reduction functions (cm_sum etc) whose behavior is sensitive to the
  // surrounding SIMD CF context. It is lowered by the CMSimdCFLowering pass.
  //
  def int_genx_simdcf_predicate :
              Intrinsic<[llvm_anyvector_ty],
                        [LLVMMatchType<0>, LLVMMatchType<0>], []>;

  // llvm.genx.simdcf.any : simd cf marker intrinsic.
  //
  // arg0: vector of i1
  //
  // Return value: i1 value as condition for a scalar control flow.
  //
  // This intrinsic is used to mark a simd cf that takes a predicate vector and
  // returns a scalar value for scalar cf.
  //
  // This is generated by clang codegen in the implementation of SIMD control
  // flow, and lowered by the CMSimdCFLowering pass.
  //
  def int_genx_simdcf_any :
              Intrinsic<[llvm_i1_ty], [llvm_anyvector_ty], []>;

  // ``llvm.genx.local.*`` : read local ID register
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  // * ``llvm.genx.local.id`` : read implicit arg local_id 
  // * ``llvm.genx.local.size`` : read implicit arg local_size
  //
  // * Return value:  v3i32 - allows for x, y and z components
  //
  // This is generated by clang codegen and lowered by CMImpParam.
  //
  def int_genx_local_id : Intrinsic<[llvm_anyvector_ty] , [], [IntrNoMem]>;
  def int_genx_local_size : Intrinsic<[llvm_anyvector_ty], [], [IntrNoMem]>;

  // ``llvm.genx.group.count`` : read group count register
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  // ``llvm.genx.group.count`` : read vISA v9 (%group_count_x)
  //
  // * Return value:  3xi32 the value read (allows for x, y and z components)
  //
  // This is generated by clang codegen and lowered by CMImpParam.
  //
  def int_genx_group_count : Intrinsic<[llvm_anyvector_ty], [], [IntrNoMem]>;

  // ``llvm.genx.get.scoreboard.bti`` : get scoreboard surface implicit
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // Return Value: i32 the surfaceindex of the scoreboard bti
  //
  // This is generated by clang codegen and lowered by CMImpParam.
  //
  def int_genx_get_scoreboard_bti : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  // ``llvm.genx.get.scoreboard.deltas`` : get scoreboard deltas
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // Return Value: vector of 16 i8 values (8 x and 8 y)
  //
  // This is generated by clang codegen and lowered by CMImpParam.
  //
  def int_genx_get_scoreboard_deltas : Intrinsic<[llvm_v16i8_ty], [], [IntrNoMem]>;

  // ``llvm.genx.get.scoreboard.depcnt`` : get the maximal scoreboard dependency count
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // Return Value: i32
  //
  // This is generated by clang codegen and lowered by CMImpParam.
  //
  def int_genx_get_scoreboard_depcnt : Intrinsic<[llvm_i32_ty], [], [IntrNoMem]>;

  // ``llvm.genx.predefined.surface`` : get predefined surface
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // Return Value: surface index of the specified id.
  //
  // This is generated by clang codegen when predefined surface is accessed.
  //
  def int_genx_predefined_surface : Intrinsic<[llvm_i32_ty], [llvm_i32_ty], [IntrNoMem]>;

  //--------------------------------------------------------------------
  // GenX backend internal intrinsics

  // llvm.genx.constanti : copy constant to register
  // llvm.genx.constantf : copy constant to register
  //
  // arg0: input value (constant, any scalar or vector type other than i1 or
  //         vector of i1)
  //
  // Return value: same type
  //
  // This intrinsic is inserted by the GenXLowering pass
  // to load a constant in a way that stops the subsequent CSE pass
  // from propagating it back into the operand using it.
  //
  // There are two variants simply because there is no way of saying here
  // that an argument can have any scalar or vector type.
  //
  def int_genx_constanti : Intrinsic<[llvm_anyint_ty],
          [LLVMMatchType<0>], [IntrNoMem]>;
  def int_genx_constantf : Intrinsic<[llvm_anyfloat_ty],
          [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.convert : convert register category (non address)
  //
  // arg0: input value (i32 or vector of i32)
  //
  // Return value: converted value (same type)
  //
  // This intrinsic is inserted by the GenXCatgory pass to represent
  // a value being converted between two register categories. The input and
  // result categories are not represented; they are implied by the other
  // def/uses of the value. Address conversion is not covered by this
  // intrinsic.
  //
  // The intrinsic is also inserted by GenXCoalescing to represent a copy
  // of a value of category other than general. Thus the input and output
  // might be both the same category, but not both general.
  //
  def int_genx_convert :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.convert.addr : convert to address register category
  //
  // arg0: input value (i16 or vector of i16)
  // arg1: constant offset (i16)
  //
  // Return value: converted value (same type)
  //
  // This intrinsic is inserted by the GenXCatgoryConversion pass to represent
  // a value being converted from a general value to an address, used as the
  // variable index in an element or region access. There it is created with
  // offset set to 0; GenXAddressCommoning may adjust that offset to try and
  // stop the address conversion falling outside of the register into which it
  // points to avoid going out of spec (bug 4395).
  //
  def int_genx_convert_addr :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>, llvm_i16_ty],
                [IntrNoMem]>;

  // llvm.genx.constantpred : load constant predicate (i1 or vector of i1)
  //
  // arg0: constant i1 or vector of i1
  //
  // Return value: loaded value, same type
  //
  // This intrinsic is inserted by GenXLowering to load a predicate constant.
  // We could just use a bitcast, except that EarlyCSE follows
  // GenXConstantMaterialization and it has a habit of putting the constant
  // back in the wrregion.
  def int_genx_constantpred :
              Intrinsic<[llvm_anyint_ty], [LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.add.addr : add an offset onto an address register
  //
  // arg0: lhs input (i16 or vector of i16)
  // arg1: rhs input (i16 or vector of i16)
  //
  // Return value: result of add (same type with arg1)
  //
  // When the result of a constant add/sub is used as a variable index in
  // a region access, GenXCategoryConversion converts it into this intrinsic
  // so that it will be considered an add to an address register.
  //
  def int_genx_add_addr :
            Intrinsic<[llvm_anyint_ty],
                      [llvm_anyint_ty, LLVMMatchType<0>], [IntrNoMem]>;

  // llvm.genx.rdpredregion : read region at specified offset from a predicate
  //
  // arg0: i1 vector
  // arg1: constant i32 offset (in elements)
  //
  // Return value: v4i1/v8i1/v16i1 result of region read
  //
  // The number of elements to read is determined from the number of elements
  // in the return type, and must be 4, 8 or 16.
  // The offset must be a multiple of the number of elements.
  //
  def int_genx_rdpredregion : Intrinsic<[llvm_anyint_ty],
      [llvm_anyint_ty, llvm_i32_ty], [IntrNoMem]>;

  // llvm.genx.wrpredregion : write region at specified offset into a predicate
  //
  // arg0: i1 old value of vector
  // arg1: i1 subvector to write into region
  // arg2: constant i32 offset (in elements)
  //
  // Return value: v4i1/v8i1/v16i1 result of region write
  //
  // The number of elements to write is determined from the number of elements
  // in the "subvector to write" arg, and must be 4, 8 or 16.
  // The offset must be a multiple of the number of elements.
  //
  def int_genx_wrpredregion : Intrinsic<[llvm_anyint_ty],
      [LLVMMatchType<0>, llvm_anyint_ty, llvm_i32_ty], [IntrNoMem]>;

  // llvm.genx.wrpredpredregion : predicated write region at specified offset
  // into a predicate
  //
  // arg0: vXi1 old value of vector
  // arg1: vYi1 subvector to write into region
  // arg2: constant i32 offset (in elements)
  // arg3: vXi1 predicate
  //
  // Return value: vXi1 result of region write
  //
  // The number of elements to write is determined from the number of elements
  // in the "subvector to write" arg, and must be 4, 8 or 16.
  // The offset must be a multiple of the number of elements.
  //
  // The constant offset indexes both the vector itself and the predicate. This
  // intrinsic is valid only if the predicate is an EM value, and the subvector
  // operand is the result of a cmp (which is then baled in).
  //
  def int_genx_wrpredpredregion : Intrinsic<[llvm_anyint_ty],
      [LLVMMatchType<0>, llvm_anyint_ty, llvm_i32_ty, LLVMMatchType<0>],
      [IntrNoMem]>;

  // ``llvm.genx.wrconstregion`` : write a constant region
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // * arg0: vector to write region in to
  // * arg1: subvector to write into the region, constant
  // * arg2: i32 vstride in elements, constant
  // * arg3: i32 width in elements, constant
  // * arg4: i32 stride in elements, constant
  // * arg5: i16 or vXi16 offset in bytes, constant
  // * arg6: i32 parent width, constant, ignored
  // * arg7: constant scalar i1 predicate value 1
  //
  // * Return value: the updated vector with the region modified
  //
  // This is the same as llvm.genx.wrregion, but with the following restrictions:
  //
  // * the subvector to write is constant;
  // * the offset is constant;
  // * the predicate is 1.
  //
  // It is used by GenXConstants when inserting code to load a constant, and
  // specifically does not participate in simplification or constant
  // propagation so we do not lose that constant loading code.
  //
  // The operands are the same as llvm.genx.wrregion so it can mostly be handled
  // by the same code as llvm.genx.wrregion.
  //
  def int_genx_wrconstregion : Intrinsic<[llvm_anyvector_ty], [LLVMMatchType<0>,
      llvm_anyvector_ty, llvm_i32_ty, llvm_i32_ty, llvm_i32_ty, llvm_anyint_ty,
      llvm_i32_ty, llvm_anyint_ty], [IntrNoMem]>;

  // ``llvm.genx.output`` : Mark output arguments
  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  //
  // * Return value: void
  //
  // This implementation intrinsic is to mark a list of output arguments.
  // This intrinsic call only extends the live range of marked arguments and
  // emits no code.
  //
  def int_genx_output : Intrinsic<[], [llvm_vararg_ty], []>;

}
